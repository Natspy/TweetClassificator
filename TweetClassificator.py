# -*- coding: utf-8 -*-
"""NK_zadanie_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZhbFXV30aSQzHPxRjOzsEPdWdsdS45w

# Praca domowa I, zadanie II

## Treść

### Wstęp fabularny

Wyobraź sobie, że jesteś pracownikiem w firmie sprzedającej kompleksową usługę tworzenia wizerunków medialnych. Oddział, w którym pracujesz obsługuje ważnego klienta działającego w branży gier i usług cyfrowych.

Twoim zadaniem jest przygotować model uczenia maszynowego, który określać będzie nastawienie emocjonalne postów z Twittera. Zespół odpowiadający za zbieranie danych właśnie dostarczył zestaw danych dla Ciebie.

Do tej pory klasyfikowaniem nastrojów z twittów zajmował się zespół ekspertów. Rozwiązanie takie jest bardzo wolne i drogie, a dokładność ekspertów wynosi tylko 95%. Dlatego zarząd firmy zlecił wdrożenie modelu uczenia maszynowego.

Twój model stanowić będzie jedynie część większego produktu oferowanego przez Twoją firmę. Wyniki Twojego modelu będą bezpośrednio wykorzystywane przez następny zespół, którego zadaniem jest przygotować kolejny model uczenia maszynowego przewidujący reakcje opinii publicznej na posty klienta.

Prace zespołu, który korzystać będzie z Twojego modelu są już bardzo zaawansowane, dlatego nie może on pozwolić sobie na żadne dodatkowe zmiany w swoim projekcie. Absolutnie konieczne jest, aby Twój model przyporządkowywał posty do jednej z trzech klas 'Positive', 'Negative', 'Neutral' lub analogicznych. Posty nie na temat powinny być klasyfikowane jako 'Neutral'.

Notebook z Twoim projektem będzie oglądał Twój szef, więc koniecznie zadbaj, żeby znalazły się w nim najważniejsze przemyślenia, a rysunki były ładne.

Powodzenia 🦾

### Polecenia

1. Wstępna obróbka danych:

 - załaduj zbiór treningowy i testowy,
 - usuń wiersze o brakujących elementach,
 - w kolumnie `sentiment` zamień wartości `'Irrelevant'` na `'Neutral'`.

1. Wykonaj wizualizacje danych:

 - histogram tematów twittów (`entity`),
 - histogram nastawień (`sentiment`),
 - najczęściej padających słów w treści twittów (`content`).

1. Przygotuj dane:

 - przygotuj zbiór cech poprzez wektoryzacje kolumny `content`, 
 - przygotuj etykiety poprzez zakodowanie tekstowych wartości w kolumnie `sentiment` do postaci liczbowej.

  Następnie wytrenuj naiwny model bayesowski. Sprawdź działanie modelu na kilku własnoręcznie napisanych wiadomościach. 

1. Wytrenuj modele:
 - naiwny bayesowski,
 - liniowy SVM,
 - regresji logistycznej,
 - drzewo decyzyjne.

  Sprawdź model na danych treningowych (walidacja krzyżowa) i testowych, następnie wybierz najlepszy model. Uzasadnij swój wybór.
  
1. Zespół ekspertów ręcznie klasyfikuje dane z dokładnością 95%. Porównaj z nimi swój model i napisz jakie są przewagi Twojego modelu.

### Zbiór danych

Zbiór danych został przygotowany na podstawie zbioru [Twitter Sentiment Analysis](https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis) i składa się z dwóch plików:
-  `twitter_training.csv` - zbiór treningowy,
- `twitter_validation.csv` - zbiór testowy.

Archiwum z plikami można pobrać z [dysku google](https://drive.google.com/file/d/1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB/view?usp=sharing) lub odkomentowując poniższe linie:
"""

#! pip install gdown
! gdown https://drive.google.com/uc?id=1sw2vA87fmAI5V5Xl9k-PCSdN5XwydhOB
! unzip twitter.zip

"""# Rozwiązanie - Natalia Karczewska 418376
---

##1. Wstępna obróbka danych
---
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Tworzenie obiektów pd.DataFrame na podstawie plików twitter_training.csv oraz twitter_validation.csv

df_train = pd.read_csv("/content/twitter_training.csv", sep=",")
df_test = pd.read_csv("/content/twitter_validation.csv", sep=",")
df_train.head()

df_test.head()

print("Kształt zbioru treningowego:", df_train.shape)
print("Kształt zbioru testowego:", df_test.shape)

# Usunięcie wierszy zawierających wartości nieokreślone 'NaN'

df_train = df_train.dropna(axis=0)
df_test = df_test.dropna(axis=0)
print("Kształt zbioru treningowego po usunięciu pustych wierszy:", df_train.shape)
print("Kształt zbioru testowego po usunięciu pustych wierszy:", df_test.shape)

# Zmiana wartości 'Irrelevant' na 'Neutral'

df_train['sentiment'] = df_train['sentiment'].replace(['Irrelevant'],'Neutral')
df_test['sentiment'] = df_test['sentiment'].replace(['Irrelevant'],'Neutral')

"""## 2. Wizualizacja danych
---
"""

# Histogram tematów tweetów (zbiór treningowy)
fig = plt.figure(figsize = (30,4))

ax = sns.countplot(x='entity', data=df_train)
ax.set_xticklabels(labels=np.sort(df_train.entity.unique()), rotation=50, ha="right")
plt.ylabel("Liczba wystąpień tematu")
plt.grid(True, linestyle='-.', color='grey')
plt.title('Histogram tematów twittów (zbiór treningowy)')
plt.ylim(0,2800)
plt.show()

# Histogram tematów tweetów (zbiór testowy)
fig = plt.figure(figsize = (30,4))

ax2 = sns.countplot(x='entity', data=df_test)
ax2.set_xticklabels(labels=np.sort(df_test.entity.unique()), rotation=50, ha="right")
plt.ylabel("Liczba wystąpień tematu")
plt.grid(True, linestyle='-.', color='grey')
plt.title('Histogram tematów twittów (zbiór testowy)')
plt.show()

"""#### Obserwacja:
W zbiorze treningowym większość tematów tweetów występuje podobnie często, w zbiorze testowym obserwuję większe zróżnicowanie pod względem wystąpień danego tematu.
"""

# Histogramy nastawień tweetów
fig = plt.figure(figsize = (12,7))

plt.subplot(1,2,1)
labels, counts = np.unique(df_train['sentiment'], return_counts=True,)
plt.bar(labels, counts, align='center', color="lightskyblue", edgecolor='black', alpha=0.75)
plt.gca().set_xticks(labels)
plt.grid(True, linestyle='-.', color='grey')
plt.ylabel("Liczba wystąpień tweetów o danym nastawieniu")
plt.title("Histogram nastawień tweetów (zbiór treningowy)")

plt.subplot(1,2,2)
labels, counts = np.unique(df_test['sentiment'], return_counts=True,)
plt.bar(labels, counts, align='center', color="pink", edgecolor='black', alpha=0.75)
plt.gca().set_xticks(labels)
plt.grid(True, linestyle='-.', color='grey')
plt.title("Histogram nastawień tweetów (zbiór testowy)")

plt.show()

"""#### Obserwacja:
Najwięcej tweetów ma nacechowanie neutralne, natomiast pozytywnych oraz negatywnych tweetów występuje zbliżona ilość, zarówno w zbiorze treningowym, jak i w testowym.

#### Analiza częstości występowania słów w tweetach za pomocą biblioteki WordCloud

Wielkość słowa odpowiada częstości jego występowania w zbiorze danych.
"""

!pip3 install wordcloud
import wordcloud

from wordcloud import WordCloud

fig = plt.figure(figsize = (20,10))
fig.suptitle("Najczęściej występujące słowa w tweetach (zbiór treningowy)", fontsize=20, va='top')

# Słowa z klasyfikacji pozytywnej
plt.subplot(1,2,1)
positive_words = " ".join(list(df_train[df_train['sentiment']=='Positive']['content']))
positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)
plt.title("Tweety pozytywne",fontsize=15)
plt.imshow(positive_plot)

# Słowa z klasyfikacji negatywnej
plt.subplot(1,2,2)
negative_words = " ".join(list(df_train[df_train['sentiment']=='Negative']['content']))
negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)
plt.title("Tweety negatywne",fontsize=15)
plt.imshow(negative_plot)

plt.show()

fig = plt.figure(figsize = (20,10))
fig.suptitle("Najczęściej występujące słowa w tweetach (zbiór testowy)", fontsize=20, va='top')

# Słowa z klasyfikacji pozytywnej
plt.subplot(1,2,1)
positive_words = " ".join(list(df_test[df_test['sentiment']=='Positive']['content']))
positive_plot = WordCloud(width = 512, height = 512).generate(positive_words)
plt.title("Tweety pozytywne",fontsize=15)
plt.imshow(positive_plot)

# Słowa z klasyfikacji negatywnej
plt.subplot(1,2,2)
negative_words = " ".join(list(df_test[df_test['sentiment']=='Negative']['content']))
negative_plot = WordCloud(width = 512, height = 512).generate(negative_words)
plt.title("Tweety negatywne",fontsize=15)
plt.imshow(negative_plot)

plt.show()

"""## 3. Przygotowanie danych do analizy
--- 
"""

# Przygotowanie kolumny etykiet

# Zmiana wartości 'Positive' na '1'
df_train['sentiment'] = df_train['sentiment'].replace(['Positive'],int(1))
df_test['sentiment'] = df_test['sentiment'].replace(['Positive'],int(1))

# Zmiana wartości 'Negative' na '0'
df_train['sentiment'] = df_train['sentiment'].replace(['Negative'],int(0))
df_test['sentiment'] = df_test['sentiment'].replace(['Negative'],int(0))

# Zmiana wartości 'Neutral' na '2'
df_train['sentiment'] = df_train['sentiment'].replace(['Neutral'],int(2))
df_test['sentiment'] = df_test['sentiment'].replace(['Neutral'],int(2))

df_test

"""### Naiwny klasyfikator Bayesa oraz sprawdzenie jego działania na kilku własnoręcznie napisanych tweetach"""

# Trenowanie naiwnego modelu bayesowskiego oraz sprawdzenie jego działania na kilku własnoręcznie napisanych tweetach

my_test = {'sentiment': [1,1,0,0], 'content': ["Man I love this new champion in League Of Legends, he's dope",
                                               "Red Dead Redemption was the best game i've played this year",
                                               "You still can bug yourself in that wall? Fix that shit yo",
                                               "This fucking game is so full of cheaters that I rage quitted already three times this week"]}
df_my_test = pd.DataFrame(data=my_test)
df_my_test

# Trenowanie naiwnego modelu bayesowskiego
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

X_train = df_train.content
y_train = df_train.sentiment

X_my_test = df_my_test.content
y_my_test = df_my_test.sentiment 

# Wektoryzacja kolumny 'content'
vectorizer = CountVectorizer()

X_train = vectorizer.fit_transform(X_train)
X_my_test = vectorizer.transform(X_my_test)

# Tworzenie obiektu klasyfikatora
clf = MultinomialNB()

# Uczenie klasyfikatora na zbiorze uczącym
clf.fit(X_train, y_train)

# Predykcja dla ręcznie napisanych przykładów oraz wyniki
y_pred = clf.predict(X_my_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_my_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_my_test, y_pred))

print("-"*55)
print("Accuracy score:")
print(accuracy_score(y_my_test, y_pred))

"""#### Obserwacja:

Model zdaje się działać dobrze, ale musi on zostać sprawdzony na większej ilości przykładów, gdyż te napisane przeze mnie są dosyć proste (używam odpowiednich słów zaobserwowanych w wizualizacji danych), dodatkowo przykładów jest stosunkowo mało.

## 4. Trenowanie modeli uczenia maszynowego
---

### a) Konstrukcja modeli oraz sprawdzenie ich działania na zbiorze treningowym (Walidacja krzyżowa)
"""

# Podział zbioru treningowego - zestaw testowy stanowi 20% 
from sklearn.model_selection import train_test_split

X = df_train.content
y = df_train.sentiment

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Wektoryzacja kolumny 'content'
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

# Słownik na wyniki (w celu późniejszego porównania modeli)
d = {}

"""#### Naiwny klasyfikator Bayesa (1)"""

# Tworzenie instancji klasyfikatora Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()

# Uczenie klasyfikatora na zbiorze uczącym
clf.fit(X_train, y_train)

# Predykcja

y_pred = clf.predict(X_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['bayes(1)'] = ACC

"""####Drzewa decyzyjne (1)"""

# Tworzenie instancji klasyfikatora Decision Tree Classifier
from sklearn import tree

clf = tree.DecisionTreeClassifier()

clf.fit(X_train, y_train)

# predykcja dla zbioru testowego oraz miary jakości
y_pred = clf.predict(X_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['drzewa(1)'] = ACC

"""#### Regresja logistyczna (1)"""

# Tworzenie instancji obiektu klasy LogisticRegression
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

# Trenowanie modelu
model.fit(X_train,y_train)

# predykcja dla zbioru testowego oraz miary jakości
y_pred = model.predict(X_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['regr(1)'] = ACC

"""#### Algorytm wektorów wspierających (SVM)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Trening klasyfikatora SVM
# from sklearn.svm import SVC
# 
# # Trenowanie modelu zajmowało trochę za długo, dlatego zmniejszam rozmiar zbioru uczącego
# X = df_train.content[:10000] 
# y = df_train.sentiment[:10000]
# 
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# 
# vectorizer = CountVectorizer()
# X_train = vectorizer.fit_transform(X_train)
# X_test = vectorizer.transform(X_test)
# 
# model = SVC()
# model.fit(X_train,y_train)
# 
# # Predykcja
# y_pred = model.predict(X_test)

# Miary jakości

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['svm(1)'] = ACC

# Zobaczmy dotychczasowe porównanie dokładności modeli
for key in d.keys():
  print(str(key) + ': ACC = ' + str(round(d[key], 3)))

"""#### Obserwacja:
Algorytm wektorów wspierających jest póki co najlepiej rokującym modelem, jego dokładność jest o niecałe 4% niższa od dokładności zespołu ekspertów ręcznie klasyfikujących dane. Nie da się ukryć, że przewagą algorytmu nad zespołem ekspertów jest znacznie szybszy czas klasyfikacji - a jak wiadomo, czas to pieniądz. Model SVM klasyfikuje poprawnie około 91% z 2000 przypadków w mniej niż 15 sekund.

### b) Konstrukcja modeli oraz sprawdzenie ich działania na zbiorze testowym
"""

# Podział danych na treningowe oraz testowe
X_train = df_train.content
y_train = df_train.sentiment

# Jako zbiór testowy wykorzystuję tym razem dane przeznaczone do walidacji z pliku twitter_validation.csv
X_test = df_test.content
y_test = df_test.sentiment

# Wektoryzacja kolumny 'content'
vectorizer = CountVectorizer()

X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

"""#### Naiwny klasyfikator Bayesa (2)"""

# Tworzenie obiektu klasyfikatora
clf = MultinomialNB()

# Uczenie klasyfikatora na zbiorze uczącym
clf.fit(X_train, y_train)

# Predykcja dla przykładów ze zbioru testowego
y_pred = clf.predict(X_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['bayes(2)'] = ACC

"""####Drzewa decyzyjne (2)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Tworzenie instancji klasyfikatora Decision Tree Classifier
# 
# clf = tree.DecisionTreeClassifier()
# 
# # Fitowanie
# clf.fit(X_train, y_train)
# 
# # predykcja dla zbioru testowego
# y_pred = clf.predict(X_test)

# Miary jakości

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['drzewa(2)'] = ACC

"""#### Regresja logistyczna (2)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Tworzenie instancji obiektu klasy LogisticRegression
# model = LogisticRegression(solver='lbfgs')
# 
# # Trenowanie modelu
# model.fit(X_train,y_train)
# 
# # Predykcja dla zbioru testowego
# y_pred = model.predict(X_test)

# Miary jakości

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['regr(2)'] = ACC

"""#### Algorytm wektorów wspierających (SVM) (2)"""

# Trening klasyfikatora SVM

X = df_train.content[:10000] # Trenowanie modelu zajmowało trochę za długo, dlatego zmniejszam rozmiar zbioru uczącego
y = df_train.sentiment[:10000]
X_test = df_test.content

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(X)
X_test = vectorizer.transform(X_test)

model = SVC()
model.fit(X, y)

# Tym razem przetestuję model SVM na danych walidacyjnych
y_pred = model.predict(X_test)

print("-"*55)
print("Classification report:")
print(classification_report(y_test, y_pred, zero_division=0))

print("-"*55)
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

print("-"*55)
print("Accuracy score:")
ACC = accuracy_score(y_test, y_pred)
print(ACC)

d['svm(2)'] = ACC

# Zobaczmy porównanie dokładności modeli po przetestowaniu ich danymi przeznaczonymi do walidacji
for key in d.keys():
  print(str(key) + ': ACC = ' + str(round(d[key], 3)))

"""#### Obserwacja:
Ostateczna weryfikacja modeli nieco zmienia sytuację. Algorytm wektorów wspierających okazał się nie być tak dokładny, jak wcześniej przypuszczano - być może we wcześniejszym przypadku mieliśmy do czynienia z przeuczeniem - model osiągał dobre wyniki gdy jego działanie sprawdzano na danych pochodzących z tego samego zbioru treningowego, jednak daje on znacznie gorsze wyniki, gdy zastosuje się go do danych, z którymi nie zetknął się podczas uczenia. Warto jednak zauważyć, że w przypadku reszty modeli, metryki polepszyły się znacząco - na prowadzenie wysuwa się regresja logistyczna.

## 5. Podsumowanie i wnioski
---

* Modele uczenia maszynowego przetestowano na dwa sposoby: 
> 1. Zastosowano prostą walidację krzyżową przeprowadzoną z wykorzystaniem danych treningowych z pliku twitter_training.csv - Zbiór danych podzielono na zbiór uczący oraz zbiór testowy korzystając z funkcji *train_test_split()* z modułu *sklearn.model_selection*. Rozmiar zbioru testowego stanowił 20% rozmiaru zbioru uczącego. Modele wytrenowano na zbiorze uczącym, a następnie sprawdzono ich działanie na zbiorze testowym.
> 2. Dane z pliku twitter_training.csv wykorzystano w całości jako zbiór uczący, natomiast dane z pliku twitter_validation.csv wykorzystano jako zbiór testowy. Ponownie, modele wytrenowano na zbiorze uczącym, natomiast działanie modeli sprawdzono na zbiorze testowym, który tym razem składał się bezpośrednio z przykładów przeznaczonych do walidacji.

* Poza modelem SVM, wszystkie modele radzą sobie lepiej z klasyfikacją testowaną sposobem numer 2.

* Zwracając uwagę zarówno na metryki modeli jak i na czas potrzebny do wytrenowania oraz zastosowania modelu, zdecydowanie najlepiej prezentuje się model regresji logistycznej. Model potrzebuje niecałych 12 sekund na trening oraz poprawne sklasyfikowanie około 94% z 1000 przykładów. Dokładność modelu jest niemalże tak samo wysoka, jak dokładność ekspertów klasyfikujących dane ręcznie. Na drugim miejscu znajdują się drzewa decyzyjne, klasyfikujące dane z dokładnością około 93%, potrzebujące jednak trochę dłuższego czasu na trening oraz klasyfikację (około 36s). Zatem wracając do modelu regresji logistycznej, należy zwrócić uwagę na to, że ręczna klasyfikacja 1000 przykładów przez zespół ekspertów zajmuje nieporównywalnie więcej czasu, niż 12 sekund. Zatem wybrany model nie tylko spisuje się w porównywalny sposób pod względem dokładności, ale ma on również ogromną przewagę pod względem czasu potrzebnego na klasyfikację.
"""